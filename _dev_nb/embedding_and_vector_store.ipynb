{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from src.configs.env_config import config\n",
    "from src.services.db import chroma_service\n",
    "from pathlib import Path\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from pprint import pprint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from src.services.utils import (\n",
    "    text_splitter_recursive_char,\n",
    "    create_chunk_ids,\n",
    "    json_to_documents,\n",
    ")\n",
    "from src.services.processors import DocumentsPreprocessing\n",
    "from src.services.vectorstore import ChromaStore\n",
    "from src.services.retrievers import MultiQRerankedRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chroma_service()\n",
    "client.heartbeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"local_collection\"\n",
    "# collection_name = \"knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.get_collection(collection_name):\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data_src = Path(\"_dev_nb/output_data/pdf_loader\")\n",
    "web_data_src = Path(\"_dev_nb/output_data/web_loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = web_data_src / \"setics_stad_docs_clean.json\"\n",
    "docs = json_to_documents(filename=json_path)\n",
    "\n",
    "print(f\"Got {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_copy1 = docs.copy()\n",
    "random.shuffle(docs_copy1)\n",
    "split_idx1 = random.randint(100, len(docs_copy1) - 1)\n",
    "docs_part1 = docs_copy1[:split_idx1]\n",
    "\n",
    "docs_copy2 = docs.copy()\n",
    "random.shuffle(docs_copy2)\n",
    "split_idx2 = random.randint(100, len(docs_copy2) - 1)\n",
    "docs_part2 = docs_copy2[:split_idx2]\n",
    "\n",
    "print(f\"Dataset 1 -> Part 1: {len(docs_part1)} | Part 2: {len(docs_part2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DocumentsPreprocessing()\n",
    "chunks1, ids1 = await processor(documents=docs_part1)\n",
    "chunks2, ids2 = await processor(documents=docs_part2)\n",
    "\n",
    "print(f\"Created {len(chunks1)} and {len(chunks2)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ChromaStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_count, skipped_count, skipped_sources = await store.add_documents(\n",
    "    documents=chunks1, ids=ids1, collection_name=collection_name\n",
    ")\n",
    "\n",
    "print(f\"Added {added_count} chunks to the collection\")\n",
    "print(f\"Skipped {skipped_count} chunks\")\n",
    "print(f\"Skipped sources: {skipped_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_count, skipped_count, skipped_sources = await store.add_documents(\n",
    "    documents=chunks2, ids=ids2, collection_name=collection_name\n",
    ")\n",
    "\n",
    "print(f\"Added {added_count} chunks to the collection\")\n",
    "print(f\"Skipped {skipped_count} chunks\")\n",
    "print(f\"Skipped sources: {skipped_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DocumentsPreprocessing()\n",
    "chunks, ids = await processor(documents=docs)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_count, skipped_count, skipped_sources = await store.add_documents(\n",
    "    documents=chunks, ids=ids, collection_name=collection_name\n",
    ")\n",
    "\n",
    "print(f\"Added {added_count} chunks to the collection\")\n",
    "print(f\"Skipped {skipped_count} chunks\")\n",
    "print(f\"Skipped sources: {skipped_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(collection_name).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the purpose of the Advanced Designer?\"\n",
    "# query = \"What can you tell me about underground fiber architecture for Xplore?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQRerankedRetriever()\n",
    "results = await retriever(query=query, collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=config.OPENAI_API_KEY,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping base retriever with FlashRank compressor\n",
    "\n",
    "# create MultiQueryRetriever\n",
    "base_retriever = await store.get_retriever(collection_name=collection_name)\n",
    "# base_retriever = store.as_retriever(search_kwargs={\"k\": 10})\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\", top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=multi_query_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prototyping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=config.OPENAI_API_KEY,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_json_path = pdf_data_src / \"xplore_pdf_3_clean.json\"\n",
    "pdf_docs = json_to_documents(filename=pdf_json_path)\n",
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_json_path = web_data_src / \"setics_stad_docs_clean.json\"\n",
    "web_docs = json_to_documents(filename=web_json_path)\n",
    "len(web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_json_path_2 = web_data_src / \"setics_stpl_docs_clean.json\"\n",
    "web_docs_2 = json_to_documents(filename=web_json_path_2)\n",
    "len(web_docs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_json_path = web_data_src / \"setics_stad_img_docs.json\"\n",
    "img_docs = json_to_documents(filename=img_json_path)\n",
    "len(img_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(pdf_docs):\n",
    "#     print(f\"Doc {i}: length {len(doc.page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks = text_splitter_recursive_char(pdf_docs)\n",
    "len(pdf_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(web_docs):\n",
    "#     print(f\"Doc {i}: length {len(doc.page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_chunks = text_splitter_recursive_char(web_docs)\n",
    "len(web_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_chunks_2 = text_splitter_recursive_char(web_docs_2)\n",
    "len(web_chunks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, doc in enumerate(web_chunks):\n",
    "#     print(f\"Doc {i}: length {len(doc.page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks_ids = create_chunk_ids(pdf_chunks)\n",
    "web_chunks_ids = create_chunk_ids(web_chunks)\n",
    "web_chunks_2_ids = create_chunk_ids(web_chunks_2)\n",
    "\n",
    "img_ids = [i.metadata[\"id\"] for i in img_docs]\n",
    "\n",
    "print(\n",
    "    pdf_chunks_ids[:2], web_chunks_ids[:2], web_chunks_2_ids[:2], img_ids[:2], sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(web_chunks[10].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\", openai_api_key=config.OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection.name,\n",
    "    embedding_function=openai_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_ids = [\n",
    "    (web_chunks, web_chunks_ids),\n",
    "    (pdf_chunks, pdf_chunks_ids),\n",
    "    (web_chunks_2, web_chunks_2_ids),\n",
    "    (img_docs, img_ids),\n",
    "]\n",
    "\n",
    "for docs, ids in documents_with_ids:\n",
    "    vector_store.add_documents(documents=docs, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vector_store.as_retriever(\n",
    "#     search_type=\"mmr\",\n",
    "#     # search_type=\"similarity_score_threshold\",\n",
    "#     # search_kwargs={\"k\": 3, \"score_threshold\": 0.5},\n",
    "#     search_kwargs={\"k\": 3},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = MultiQueryRetriever.from_llm(retriever=vector_store.as_retriever(), llm=llm)\n",
    "\n",
    "# retriever = SelfQueryRetriever.from_llm(\n",
    "#     llm=llm,\n",
    "#     vectorstore=vector_store,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapping base retriever with FlashRank compressor\n",
    "\n",
    "# create MultiQueryRetriever\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)\n",
    "\n",
    "# add reranker on top\n",
    "compressor = FlashrankRerank(top_n=3)\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=multi_query_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the installation requirement for flower pot?\"\n",
    "# query = \"What can you tell me about Setics Sttar?\"\n",
    "# query = \"In Sttar, how to add a new infrastructure layer?\"\n",
    "# query = \"In Sttar, how to manually split some lines in the interface, in the map view?\"\n",
    "# query = \" In sttar, how can we manage the support properties, for the reusable infrastructure?\"\n",
    "query = \" What is the differences between the advanced designer and the planner?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set logging for the queries\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig()\n",
    "# logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.invoke(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in results:\n",
    "#     print(result.page_content)\n",
    "#     print(\"\\n\\n===\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    api_key=config.OPENAI_API_KEY,\n",
    "    # max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in results)\n",
    "\n",
    "messages = prompt.invoke({\"question\": query, \"context\": docs_content})\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(collection_name)\n",
    "collection.get(\n",
    "    ids=results[0].metadata[\"id\"], include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
